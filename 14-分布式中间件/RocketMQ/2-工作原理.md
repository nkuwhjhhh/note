## 一、消息的生产

#### 1. 消息的生产过程

1. Producer发送消息之前，会先向NameServer发出获取消息Topic的路由信息的请求
2. NameServer返回该Topic的路由表及Broker列表
3. Producer根据代码中指定的Queue选择策略，从Queue列表中选出一个队列，用于后续存储消息
4. Produer对消息做一些特殊处理，例如，消息本身超过4M，则会对其进行压缩
5. Producer向选择出的Queue所在的Broker发出RPC请求，将消息发送到选择出的Queue



路由表：Map，其中key是topic名称，value是一个QueueData实例列表。

Broker列表：Map，其中key是brokerName，value是BrokerData；BrokerData中包含的是brokerName和一个map，这个map中key为brokerId，value为该broker对应的地址。所以，应该是”双重“map。



#### 2. Queue选择算法

也称为消息投递算法

- 轮询算法
  - 可以保证每个queue均匀获取消息。
  - 但是如果有的queue上投递延迟严重，则会导致Producer的缓存队列中出现较大的消息积压，影响消息的投递性能。
- 最小投递延迟算法
  - 该算法会统计每次消息投递的时间延迟，然后根据统计出的结果将消息投递到时间延迟最小的Queue。 如果延迟相同，则采用轮询算法投递。该算法可以有效提升消息的投递性能。
  - 消息在Queue上的分配不均匀。投递延迟小的Queue其可能会存在大量 的消息。而对该Queue的消费者压力会增大，降低消息的消费能力，可能会导致MQ中消息的堆积。



## 二、消息的存储

- abort：该文件在Broker启动后会自动创建，正常关闭Broker，该文件会自动消失。若在没有启动 Broker的情况下，发现这个文件是存在的，则说明之前Broker的关闭是非正常关闭。
- checkpoint：其中存储着commitlog、consumequeue、index文件的最后刷盘时间戳
- commitlog文件夹：其中存放着commitlog文件，而消息是写在commitlog文件中的
- config文件夹：存放着Broker运行期间的一些配置数据
- consumequeue文件夹：其中存放着consumequeue文件，队列就存放在这个目录中
- index文件夹：其中存放着消息索引文件indexFile
- lock：运行期间使用到的全局资源锁



#### 1. commitlog文件夹

- 目录与文件
  - 存放很多的mappedFile文件，文件大小为1G，文件名由20位十进制数构成），表示当前文件的第一条消息的起始位移偏移量。一个Broker中所有mappedFile文件的commitlog offset是连续。
  - 消息在Broker中存放时并没有被按照Topic进行分类存放，都是按顺序写到mappedFile文件中的。
  - mappedFile文件是顺序读写的文件，所以其访问效率很高。
- 消息单元
  - ![image-20240509214357437](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240509214357437.png) 
  - mappedFile文件内容由一个个的消息单元构成。每个消息单元中包含消息总长度MsgLen、消息的物理 位置physicalOffset、消息体内容Body、消息体长度BodyLength、消息主题Topic、Topic长度TopicLength、消息生产者BornHost、消息发送时间戳BornTimestamp、消息所在的队列QueueId、消息在Queue中存储的偏移量QueueOffset等近20余项消息相关属性。
  - 一个mappedFile文件中第m+1个消息单元的commitlog offset偏移：L(m+1) = L(m) + MsgLen(m) (m >= 0)



#### 2. consumequeue文件夹

为了提高效率，会为每个Topic在consumequeue中创建一个目录，目录名为Topic名称。在该Topic目录下，会再为每个该Topic的Queue建立一个目录，目录名为queueId。每个目录中存放着若干consumequeue文件，consumequeue文件是commitlog的索引文件，可以根据consumequeue定位到具体的消息。

consumequeue文件名也由20位数字构成，表示当前文件的第一个索引条目的起始位移偏移量。与mappedFile文件名不同的是，其后续文件名是固定的。因为consumequeue文件大小是固定不变的。

##### 索引条目

<img src="C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240509215239549.png" alt="image-20240509215239549" style="zoom:50%;" /> 

每个consumequeue文件可以包含30w个索引条目，每个索引条目包含了三个消息重要属性：消息在 mappedFile文件中的偏移量CommitLog Offset、消息长度、消息Tag的hashcode值。这三个属性占20 个字节，所以每个文件的大小是固定的30w * 20字节。

一个consumequeue文件中所有消息的Topic一定是相同的。但每条消息的Tag可能是不同的。



#### 3. 对文件的读写

![image-20240509220528922](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240509220528922.png)

这里还不太明白？？？？？

https://www.bilibili.com/video/BV1fF411K7CL/



## 三、indexFile

除了通过通常的指定Topic进行消息消费外，RocketMQ还提供了根据key进行消息查询的功能。该查询 是通过store目录中的index子目录中的indexFile进行索引实现的快速查询。当然，这个indexFile中的索 引数据是在包含了key的消息被发送到Broker时写入的。如果消息中没有包含key，则不会写入。

#### 1. 索引条目结构

每个Broker中会包含一组indexFile，每个indexFile都是以一个时间戳命名的（这个indexFile被创建时 的时间戳）。每个indexFile文件由三部分构成：indexHeader，slots槽位，indexes索引数据。每个 indexFile文件中包含500w个slot槽。而每个slot槽又可能会挂载很多的index索引单元。

<img src="C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240511195126282.png" alt="image-20240511195126282" style="zoom:67%;" /> 

indexHeader固定40个字节，其中存放着如下数据：

![image-20240511195258031](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240511195258031.png)

- beginTimestamp：该indexFile中第一条消息的存储时间
- endTimestamp：该indexFile中最后一条消息存储时间
- beginPhyoffset：该indexFile中第一条消息在commitlog中的偏移量commitlog offset
- endPhyoffset：该indexFile中最后一条消息在commitlog中的偏移量commitlog offset
- hashSlotCount：已经填充有index的slot数量（并不是每个slot槽下都挂载有index索引单元，这里统计的是所有挂载了index索引单元的slot槽的数量）
- indexCount：该indexFile中包含的索引单元个数（统计出当前indexFile中所有slot槽下挂载的所 有index索引单元的数量之和）

![image-20240511200710183](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240511200710183.png)



## 四、消息的消费

#### 4.1 消费者获取消费类型

- 拉取式消费
- 推送式消费
- 二者对比
  - pull：需要应用去实现对关联Queue的遍历，实时性差；但便于应用控制消息的拉取
  - push：封装了对关联Queue的遍历，实时性强，但会占用较多的系统资源



#### 4.2 消费者组消费模式

- 广播消费：相同Consumer Group的每个Consumer实例都接收同一个Topic的全量消息。即每条 消息都会被发送到Consumer Group中的每个Consumer。
  - ![image-20240509232542170](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240509232542170.png)
- 集群消费（负载均衡消费）：相同Consumer Group的每个Consumer实例平均分摊同一个Topic的消息。即每条消息只会被发送到Consumer Group中的某个Consumer。
  - ![image-20240509232605922](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240509232605922.png)
- 二者消息进度的保存
  - 广播模式：消费进度保存在consumer端。因为广播模式下consumer group中每个consumer都会 消费所有消息，但它们的消费进度是不同。所以consumer各自保存各自的消费进度。
  - 集群模式：消费进度保存在broker中。consumer group中的所有consumer共同消费同一个Topic 中的消息，同一条消息只会被消费一次。消费进度会参与到了消费的负载均衡中，故消费进度是 需要共享的。下图是broker中存放的各个Topic的各个Queue的消费进度。
- 一般使用集群模式消费。一些特殊场景可以用到广播模式，比如打车系统中，派单服务将订单发送给每个司机都有的每个接单服务，根据相关算法判断该司机是否能够接单。



#### 4.3 Rebalance机制

集群消费下的再均衡机制。指的是，将⼀个Topic下的多个Queue在同⼀个Consumer Group中的多个 Consumer间进行重新分配的过程。

![image-20240510112704467](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510112704467.png)

- 为了提升并行消费能力
- 限制：因此当某个消费者组下的消费者实例数量大于队列的数量时， 多余的消费者实例将分配不到任何队列。
- 问题：
  - 消费暂停：重新分配时，需要暂停一些正在消费队列的customer，分配部分队列给别的customer。
  - 消费重复：重新分配队列时，接到新队列的customer按照队列消费进度offset继续消费。但是offset是异步提交的，也就是说，队列在原先的customer处可能已经开始处理消息了，还没有回复ACK。
    - 同步提交：队列等待ACK。
    - 异步提交：队列不等待ACK，可以直接获取并消费下一批消息。
    - 消息按批处理，一次性读取消息的数量需要在适当范围内，过大重复消费的概率变大，过小系统性能下降。
  - 消息突刺：由于Rebalance可能导致重复消费，如果需要重复消费的消息过多，或者因为Rebalance暂停 时间过长从而导致积压了部分消息。那么有可能会导致在Rebalance结束之后瞬间需要消费很多消息。
- 发生再分配rebalance的时机
  - 消费者所订阅Topic的Queue数量发生变化
    - Broker扩容或缩容
    - Broker升级运维
    - Broker与NameServer间的网络异常
    - Queue扩容或缩容
  - 消费者组中消费者的数量发生变化
    - Consumer Group扩容或缩容
    - Consumer升级运维
    - Consumer与NameServer间网络异常
- Rebalance过程
  - 在Broker中维护着多个Map集合，这些集合中动态存放着当前Topic中Queue的信息、Consumer Group 中Consumer实例的信息。一旦发现消费者所订阅的Queue数量发生变化，或消费者组中消费者的数量 发生变化，立即向Consumer Group中的每个实例发出Rebalance通知。
  - 三个集合：（需要保存队列信息，消费者信息，消费offset信息）
    - TopicConfigManager：key是topic名称，value是TopicConfig。TopicConfig中维护着该Topic中所 有Queue的数据。
    - ConsumerManager：key是Consumser Group Id，value是ConsumerGroupInfo。 ConsumerGroupInfo中维护着该Group中所有Consumer实例数据。
    - ConsumerOffsetManager：key为Topic与订阅该Topic的Group的组合,即topic@group， value是一个内层Map。内层Map的key为QueueId，内层Map的value为该Queue的消费进度 offset。



#### 4. Queue分配算法

一个Topic中的Queue只能由Consumer Group中的一个Consumer进行消费，而一个Consumer可以同时 消费多个Queue中的消息。

四种策略

- 平均分配策略
  - 除法
- 环形平均策略
  - 按照环形顺序一个一个分配，和发牌差不多
- 一致性hash策略
  - 类似redis中的槽分配，顺时针
  - <img src="C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510124132161.png" alt="image-20240510124132161" style="zoom:50%;" /> 
- 同机房策略
  - 该算法会根据queue的部署机房位置和consumer的位置，过滤出当前consumer相同机房的queue。然后按照平均分配策略或环形平均策略对同机房queue进行分配。如果没有同机房queue，则按照平均分配策略或环形平均策略对所有queue进行分配。



几种算法对比

- 一致性hash存在问题：不如前两种分配效率高，算法复杂，还存在可能分配不平均的情况。但可以有效减少消费者组扩容缩容带来的大量Rebalance。
- 一致性hash的应用场景：消费者数量变化频繁的场景。



#### 5. 至少一次原则

RocketMQ有一个原则：每条消息必须要被成功消费一次。

那么什么是成功消费呢？Consumer在消费完消息后会向其消费进度记录器提交其消费消息的offset， offset被成功记录到记录器中，那么这条消费就被成功消费了。

> 什么是消费进度记录器？ 对于广播消费模式来说，Consumer本身就是消费进度记录器。 对于集群消费模式来说，Broker是消费进度记录器。



## 五、订阅关系的一致性

订阅关系的一致性指的是，同一个消费者组（Group ID相同）下所有Consumer实例所订阅的Topic与 Tag及对消息的处理逻辑必须完全一致。否则，消息消费的逻辑就会混乱，甚至导致消息丢失。

#### 1. 正确订阅关系

多个消费者组订阅了多个Topic，并且每个消费者组里的多个消费者实例的订阅关系保持了一致。

<img src="C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510124839524.png" alt="image-20240510124839524" style="zoom:50%;" /> 



#### 2. 错误订阅关系

一个消费者组订阅了多个Topic，但是该消费者组里的多个Consumer实例的订阅关系并没有保持一致。

![image-20240510124902444](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510124902444.png)

举例

- 订阅了不同Topic
- 订阅了不同Tag
- 订阅了不同数量的Topic（本来就不能订阅不同topic，如果符合这个规则，自然也不会出现订阅不同数量的topic了）



## 六、offset管理

消费进度offset是用来记录每个Queue的不同消费组的消费进度的。根据消费进度记录器的不同，可以 分为两种模式：本地模式和远程模式。

#### 1. offset本地管理模式

广播消费时，offset存储在消费者本地，每个消费者管理自己的进度。

Consumer在广播消费模式下offset相关数据以json的形式持久化到Consumer本地磁盘文件中，默认文 件路径为当前用户主目录下的.rocketmq_offsets/\${clientId}/\${group}/Offsets.json。 其中\${clientId}为当前消费者id，默认为ip@DEFAULT；\${group}为消费者组名称。



#### 2. offset远程管理模式

当消费模式为集群消费时，offset使用远程模式管理。因为所有Cosnumer实例对消息采用的是均衡消费，所有Consumer共享Queue的消费进度。

Consumer在集群消费模式下offset相关数据以json的形式持久化到Broker磁盘文件中，文件路径为当前用户主目录下的store/config/consumerOffset.json。

Broker启动时会加载这个文件，并写入到一个双层Map（ConsumerOffsetManager）。外层map的key为topic@group，value为内层map。内层map的key为queueId，value为offset。当发生Rebalance时， 新的Consumer会从该Map中获取到相应的数据来继续消费。

集群模式下offset采用远程管理模式，主要是为了保证Rebalance机制。



#### 3. offset用途

消费者是如何从最开始持续消费消息的？消费者要消费的第一条消息的起始位置是用户自己通过 consumer.setConsumeFromWhere()方法指定的。

在Consumer启动后，其要消费的第一条消息的起始位置常用的有三种，这三种位置可以通过枚举类型 常量设置。这个枚举类型为ConsumeFromWhere。

![image-20240510155044665](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510155044665.png)

当消费完一批消息后，Consumer会提交其消费进度offset给Broker，Broker在收到消费进度后会将其更 新到那个双层Map（ConsumerOffsetManager）及consumerOffset.json文件中，然后向该Consumer进行ACK，而ACK内容中包含三项数据：当前消费队列的最小offset（minOffset）、最大offset（maxOffset）、及下次消费的起始offset（nextBeginOffset）。



#### 4. 重试队列

当rocketMQ对消息的消费出现异常时，会将发生异常的消息的offset提交到Broker中的重试队列。系统 在发生消息消费异常时会为当前的topic@group创建一个重试队列，该队列以%RETRY%开头，到达重试时间后进行消费重试。

![image-20240510155553297](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510155553297.png)



#### 5. offset的同步提交与异步提交

集群消费模式下，Consumer消费完消息后会向Broker提交消费进度offset，其提交方式分为两种：

- 同步提交：消费者在消费完一批消息后会向broker提交这些消息的offset，然后等待broker的成功响 应。若在等待超时之前收到了成功响应，则继续读取下一批消息进行消费（从ACK中获取 nextBeginOffset）。若没有收到响应，则会重新提交，直到获取到响应。而在这个等待过程中，消费 者是阻塞的。其严重影响了消费者的吞吐量。
- 异步提交：消费者在消费完一批消息后向broker提交offset，但无需等待Broker的成功响应，可以继续 读取并消费下一批消息。这种方式增加了消费者的吞吐量。但需要注意，broker在收到提交的offset 后，还是会向消费者进行响应的。可能还没有收到ACK，此时Consumer会从Broker中直接获取 nextBeginOffset。



## 七、消费幂等

#### 1. 什么是消费幂等

幂等：若某操作执行多次与执行一次对系统产生的影响是相同的，则称该操作是幂等的。

当出现消费者对某条消息重复消费的情况时，重复消费的结果与消费一次的结果是相同的，并且多次消 费并未对业务系统产生任何负面影响，那么这个消费过程就是消费幂等的。

在互联网应用中，尤其在网络不稳定的情况下，消息很有可能会出现重复发送或重复消费。如果重复的 消息可能会影响业务处理，那么就应该对消息做幂等处理。



#### 2. 消息重复的场景分析

传递消息时可能会发生消息重复。

- 发送时消息重复
  - 当一条消息已被成功发送到Broker并完成持久化，此时出现了网络闪断，从而导致Broker对Producer应 答失败。 如果此时Producer意识到消息发送失败并尝试再次发送消息，此时Broker中就可能会出现两 条内容相同并且Message ID也相同的消息，那么后续Consumer就一定会消费两次该消息。
- 消费时消息重复
  - 消息已投递到Consumer并完成业务处理，当Consumer给Broker反馈应答时网络闪断，Broker没有接收 到消费成功响应。为了保证消息至少被消费一次的原则，Broker将在网络恢复后再次尝试投递之前已 被处理过的消息。此时消费者就会收到与之前处理过的内容相同、Message ID也相同的消息。
- Rebalance时消息重复
  - 当Consumer Group中的Consumer数量发生变化时，或其订阅的Topic的Queue数量发生变化时，会触 发Rebalance，此时Consumer可能会收到曾经被消费过的消息。



#### 3. 通用解决方案

幂等解决方案的设计中涉及到两项要素：幂等令牌，与唯一性处理。只要充分利用好这两要素，就可以设计出好的幂等解决方案。

- 幂等令牌
  - 是生产者和消费者两者中的既定协议，通常指具备唯⼀业务标识的字符串。例如，订单号、流水号。一般由Producer随着消息一同发送来的。
- 唯一性处理
  - 服务端通过采用⼀定的算法策略，保证同⼀个业务逻辑不会被重复执行成功多次。例如，对同一笔订单的多次支付操作，只会成功一次。

其实就是拿令牌，再根据令牌进行逻辑处理。



##### 3.1 解决方案

对于常见的系统，幂等性操作的通用性解决方案是：
1. 首先通过**缓存**去重。在缓存中如果已经存在了某幂等令牌，则说明本次操作是重复性操作；若缓存没有命中，则进入下一步。
2. 在唯一性处理之前，先在**数据库**中查询幂等令牌作为索引的数据是否存在。若存在，则说明本次操作为重复性操作；若不存在，则进入下一步。（这一步存在的意义：缓存有有效期，如果过期了会去数据库查询）
3. 在**同一事务**中完成三项操作：唯一性处理后，将幂等令牌写入到缓存，并将幂等令牌作为唯一索引的数据写入到DB中。

举例：支付流程。



#### 4. 消费幂等的实现

消费幂等的解决方案很简单：为消息指定不会重复的唯一标识。因为Message ID有可能出现重复的情况，所以真正安全的幂等处理，不建议以Message ID作为处理依据。最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息Key设置。

以支付场景为例，可以将消息的Key设置为订单号，作为幂等处理的依据。

消费者收到消息时可以根据消息的Key即订单号来实现消费幂等：



## 八、消息堆积与消费延迟

#### 1. 概念

消息处理流程中，如果Consumer的消费速度跟不上Producer的发送速度，MQ中未处理的消息会越来越多（进的多出的少），这部分消息就被称为堆积消息。消息出现堆积进而会造成消息的消费延迟。

- 业务系统上下游能力不匹配造成的持续堆积，且无法自行恢复。 
- 业务系统对消息的消费实时性要求较高，即使是短暂的堆积造成的消费延迟也无法接受。



#### 2. 产生原因分析

![image-20240510161618527](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20240510161618527.png)

##### 2.1 消息拉取

Consumer通过长轮询Pull模式批量拉取的方式从服务端获取消息，将拉取到的消息缓存到本地缓冲队列中。对于拉取式消费，在内网环境下会有很高的吞吐量，所以这一阶段一般不会成为消息堆积的瓶颈。



##### 2.2 消息消费

Consumer将本地缓存的消息提交到消费线程中，使用业务消费逻辑对消息进行处理，处理完毕后获取到一个结果。这是真正的消息消费过程。此时Consumer的消费能力就完全依赖于消息的消费耗时和消费并发度了。如果由于业务处理逻辑复杂等原因，导致处理单条消息的耗时较长，则整体的消息吞吐量肯定不会高，此时就会导致Consumer本地缓冲队列达到上限，停止从服务端拉取消息。



##### 2.3 结论

消息堆积的主要瓶颈在于客户端的消费能力，而消费能力由==消费耗时==和==消费并发度==决定。注意，消费耗时的优先级要高于消费并发度。即在保证了消费耗时的合理性前提下，再考虑消费并发度问题。



#### 3. 消费耗时

影响消息处理时长的主要因素是代码逻辑。而代码逻辑中可能会影响处理时长代码主要有两种类型： CPU内部计算型代码和外部I/O操作型代码。

通常情况下代码中如果没有复杂的递归和循环的话，内部计算耗时相对外部I/O操作来说几乎可以忽略。所以**外部IO型代码**是影响消息处理时长的主要症结所在。

> 外部IO操作型代码举例：
>
> - 读写外部数据库，例如对远程MySQL的访问
> - 读写外部缓存系统，例如对远程Redis的访问
> - 下游系统调用，例如Dubbo的RPC远程调用，Spring Cloud的对下游系统的Http接口调用
>
> 关于下游系统调用逻辑需要进行提前梳理，掌握每个调用操作预期的耗时，这样做是为了能够 判断消费逻辑中IO操作的耗时是否合理。通常消息堆积是由于下游系统出现了服务异常或达到 了DBMS容量限制，导致消费耗时增加。
>
> - 服务异常，并不仅仅是系统中出现的类似500这样的代码错误，而可能是更加隐蔽的问题。例如，网络带宽问题。
> - 达到了DBMS容量限制，其也会引发消息的消费耗时增加。



#### 4. 消费并发度

一般情况下，消费者端的消费并发度由单节点线程数和节点数量共同决定，其值为单节点线程数*节点数量。不过，通常需要优先调整单节点的线程数，若单机硬件资源达到了上限，则需要通过横向扩展来提高消费并发度。

> 单节点线程数，即单个Consumer所包含的线程数量
>
> 节点数量，即Consumer Group所包含的Consumer数量

> 对于普通消息、延时消息及事务消息，并发度计算都是单节点线程数*节点数量。但对于顺序消息则是不同的。顺序消息的消费并发度等于Topic的Queue分区数量。
>
> 1. 全局顺序消息：该类型消息的Topic只有一个Queue分区。其可以保证该Topic的所有消息被顺序消费。为了保证这个全局顺序性，Consumer Group中在同一时刻只能有一个Consumer的一个线程进行消费。所以其并发度为1。
> 2. 分区顺序消息：该类型消息的Topic有多个Queue分区。其仅可以保证该Topic的每个Queue分区中的消息被顺序消费，不能保证整个Topic中消息的顺序消费。为了保证这个分区顺序性， 每个Queue分区中的消息在Consumer Group中的同一时刻只能有一个Consumer的一个线程进行消费。即，在同一时刻最多会出现多个Queue分区有多个Consumer的多个线程并行消费。所以其并发度为Topic的分区数量。



#### 5. 单机线程数计算

对于一台主机中线程池中线程数的设置需要谨慎，不能盲目直接调大线程数，设置过大的线程数反而会 带来大量的线程切换的开销。理想环境下单节点的最优线程数计算模型为：C *（T1 + T2）/ T1。

C：CPU内核数 T1：CPU内部逻辑计算耗时 T2：外部IO操作耗时

> 注意，该计算出的数值是理想状态下的理论数据，在生产环境中，不建议直接使用。而是根据当前环境，先设置一个比该值小的数值然后观察其压测效果，然后再根据效果逐步调大线程数，直至找到在该环境中性能最佳时的值.



#### 6. 如何避免出现消息堆积和消费延迟问题

为了避免在业务使用时出现非预期的消息堆积和消费延迟问题，需要在前期设计阶段对整个业务逻辑进行完善的排查和梳理。其中最重要的就是梳理消息的消费耗时和设置消息消费的并发度。

##### 6.1 梳理消息的消费耗时

通过压测获取消息的消费耗时，并对耗时较高的操作的代码逻辑进行分析。梳理消息的消费耗时需要关注以下信息：

- 消息消费逻辑的计算复杂度是否过高，代码是否存在无限循环和递归等缺陷。
- 消息消费逻辑中的I/O操作是否是必须的，能否用本地缓存等方案规避。 
- 消费逻辑中的复杂耗时的操作是否可以做异步化处理。如果可以，是否会造成逻辑错乱。



##### 6.2 设置消费并发度

对于消息消费并发度的计算，可以通过以下两步实施：

- 逐步调大单个Consumer节点的线程数，并观测节点的系统指标，得到单个节点最优的消费线程数和消息吞吐量。 
- 根据上下游链路的流量峰值计算出需要设置的节点数

> 节点数 = 流量峰值 / 单个节点消息吞吐量



## 九、消息的清理

消息被消费过后会被清理掉吗？不会的。

消息是被顺序存储在commitlog文件的，且消息大小不定长，所以消息的清理是不可能以消息为单位进 行清理的，而是以commitlog文件为单位进行清理的。否则会急剧下降清理效率，并实现逻辑复杂。

commitlog文件存在一个过期时间，默认为72小时，即三天。除了用户手动清理外，在以下情况下也会被自动清理，无论文件中的消息是否被消费过：

- 文件过期，且到达清理时间点（默认为凌晨4点）后，自动清理过期文件
- 文件过期，且磁盘空间占用率已达过期清理警戒线（默认75%）后，无论是否达到清理时间点， 都会自动清理过期文件
- 磁盘占用率达到清理警戒线（默认85%）后，开始按照设定好的规则清理文件，无论是否过期。 默认会从最老的文件开始清理
- 磁盘占用率达到系统危险警戒线（默认90%）后，Broker将拒绝消息写入

需要注意以下几点：

1）对于RocketMQ系统来说，删除一个1G大小的文件，是一个压力巨大的IO操作。在删除过程中，系统性能会骤然下降。所以，其默认清理时间点为凌晨4点，访问量最小的时间。也正因如果，我们要保障磁盘空间的空闲率，不要使系统出现在其它时间点删除commitlog文件的情况。

2）官方建议RocketMQ服务的Linux文件系统采用ext4。因为对于文件删除操作，ext4要比ext3性能更好
